---
title: "Prédire la consommation électrique en France"
subtitle: Projet ML pour la prévision, encadré par Yannig Goude
output: html_document
author: Guillaume Principato et Valérie Castin
date: "15/03/22"
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


A faire :
- remettre en forme les figures

Prédire la consommation d'électricité en France est un problème central, notamment pour la gestion du parc de production d'électricité français, dont le nucléaire, énergie peu modulable, représente une part importante. Dans ce projet, nous abordons cette question, qui rentre dans le cadre de la prévision de séries temporelles, en faisant appel à un large éventail de méthodes utilisant le machine learning. La performance de nos modèles se rapproche de celle de la prévision officielle de consommation fournie par RTE.


## 1 Importation et prétraitement des données

```{r packages, echo=FALSE, include=FALSE}
rm(list=objects())

library(tidyverse)
library(readr)
library(dplyr)
library(lubridate)
library(xts)
library(mgcv)
library(mgcViz)
library(gridExtra)
library(yarrr)
library(qgam)
library(magrittr)
library(rpart)
library(party)
library(tree)
library(rpart.plot)
library(progress)
library(plotmo)
library(caret)
library(randomForest)
library(ranger)
library(opera)
library(corrplot)
library(vip)
library(dygraphs)
library(mltools)
library(data.table)
library(caret)
library(forecast)
library(RColorBrewer)



set.seed(1)

load("DONNEES/data.rda")
load("MODELES/GAM.rda")
load("MODELES/forets_ouvre.rda")
load("MODELES/bagging_gam.rda")
load("MODELES/boosting_arbres.rda")
load("MODELES/stacking_forets.rda")
load("MODELES/stacking_boosting.rda")
load("MODELES/qgam.rda")
```

Les données que nous avons utilisées sont des données EDF, qui nous ont été fournies directement par Yannig Goude. Nous avons donc pu nous concentrer sur les modèles sans avoir à nettoyer la base de données. Elles concernent la consommation électrique globale française (particuliers, entreprises et bâtiments publics confondus), échantillonnée par demi-heure de janvier 2012 à octobre 2022. Il y a en tout 24 variables, ce qui fait un total de $4.5\times 10^{6}$ observations.

### 1.1 Les principales variables du problème

Les variables que nous avons exploitées sont les suivantes.

  - Variables temporelles
    + **tod** : time of day, valeur numérique entre 0 et 1 indiquant le moment de la journée
    + **toy** : time of year, valeur numérique entre 0 et 1 indiquant le moment de l'année
    + **Month** : chaîne de caractères indiquant le mois
    + **WeekDays** : chaîne de caractères indiquant le jour de la semaine
    + **BH** : 1 si l'on est un jour férié, 0 sinon
    + **DLS** : 1 si l'on est en heure d'hiver, 0 sinon
    + **Summer_break** : variable à deux niveaux (0 et 10) pour indiquer les vacances d'été
    + **Christmas_break** : variable à deux niveaux (0 et 20) pour indiquer les vacances de Noël
    
  - Variables de consommation électrique (valeurs numériques)
    + **Load** : consommation dans toute la France échantillonnée au pas demi-heure
    + **Load.48** : consommation 24 heures (soit 48 demi-heures) avant
  - Prévisions RTE (valeurs numériques)
    + **Forecast_RTE_dayahead** : prévision de la consommation réalisée la veille (entre 18h et 20h) par RTE pour le lendemain
    + **Forecast_RTE_intraday** : prévision de la consommation réalisée une heure avant par RTE
  - Variables liées à la température (valeurs numériques)
    + **Temp** : température moyenne mesurée par un ensemble de stations météo en France métropolitaine
    + **Temp_s95** : lissage exponentiel de Temp, avec un facteur de lissage $\alpha = 0.95$
    + **Temp_s99** : lissage exponentiel de Temp, avec un facteur de lissage $\alpha = 0.99$
    + **Temp_s95_min** : minimum des températures mesurées par les différentes stations météo, après lissage avec $\alpha = 0.95$
    + **Temp_s99_min** : minimum des températures mesurées par les différentes stations météo, après lissage avec $\alpha = 0.99$
    + **Temp_s95_max** : maximum des températures mesurées par les différentes stations météo, après lissage avec $\alpha = 0.95$
    + **Temp_s99_max** : maximum des températures mesurées par les différentes stations météo, après lissage avec $\alpha = 0.99$.

### 1.2 Analyse descriptive préliminaire

Commençons par afficher la consommation électrique sur un mois, par exemple avril 2012.

```{r, echo=FALSE, include=TRUE}
knitr::include_graphics('FIGURES/avril_2012.png')
```


On observe plusieurs périodicités dans la série temporelle :

- des oscillations journalières, de période 24h, avec des pics de consommation à certaines heures
- une périodicité hebdomadaire, avec une nette différence entre les jours ouvrés et les week-ends (ceux-ci coïncidant avec une consommation plus faible).

On s'attend aussi à avoir une périodicité annuelle. Pour la voir, nous représentons la consommation entre 2012 et 2014 pour un jour de la semaine fixé (par exemple le mardi) à une heure donnée (ici 12h), sinon il y a trop de données pour que les courbes soient lisibles.

```{r, echo=FALSE, include=TRUE}
knitr::include_graphics('FIGURES/mardi_12h.png')
```

On voit que la consommation est nettement plus importante l'hiver, et on retrouve les vacances d'été et les vacances de Noël en creux.

Ces observations nous incitent à traiter différemment chaque heure de la journée, et à distinguer les jours ouvrés des jours fériés, ce que nous faisons dans la sous-partie suivante. Quelles sont ensuite les variables explicatives les plus pertinentes pour prédire la consommation ? Nous ferons une étude plus approfondie dans la partie sur les forêts aléatoires, en nous fondant sur l'importance des variables, mais nous pouvons dans un premier temps afficher la corrélation entre les variables.

```{r, echo = FALSE, include = TRUE}
knitr::include_graphics('FIGURES/corr.png')
```

La température (telle quelle ou lissée) apparaît comme étant très anticorrélée à la consommation, ce qui est naturel : lorsqu'il fait froid, les bâtiments sont chauffés, ce qui consomme beaucoup. L'effet de la climatisation lorsqu'il fait chaud compense un peu cette tendance, mais implique une augmentation moindre de la consommation. La consommation de la veille, **Load.48**, est au contraire très corrélée à **Load**. Quant à **tod** et **toy**, elles sont assez faiblement corrélées à **Load** car la tendance s'inverse au fil de leur augmentation : par exemple pour **toy**, la corrélation est d'abord négative, car plus on s'éloigne de janvier et plus les températures remontent, induisant une baisse de la consommation, mais dès que l'automne puis l'hiver reviennent, la corrélation devient positive, et les deux effets se compensent.

### 1.3 Découpage du jeu de données

En vue d'appliquer des modèles d'apprentissage, il nous faut séparer les données qui ont été obtenues dans des conditions trop différentes, pour que l'hypothèse de données identiquement distribuées soit raisonnablement satisfaite. Tout d'abord, les données de la période covid (notamment celles de l'année 2020) ne peuvent vraisemblablement pas être traitées comme celles des années précédentes. Nous choisissons donc comme ensemble d'entraînement la période de 2012 à 2018 inclus, et l'année 2019 sert d'ensemble de validation. Nous enlevons ensuite les jours fériés, qui correspondent à un changement ponctuel de la consommation, et nous séparons les jours ouvrés des week-ends, en vertu des observations réalisées dans la sous-partie précédente. Nous allons nous concentrer dans la suite sur le cas des jours ouvrés, la prédiction pour les week-ends étant très similaire et risquant d'alourdir le rapport inutilement.

Pour avoir des modèles aussi précis que possible, mais aussi alléger le coût d'entraînement, puisque notre jeu de données est assez volumineux, nous avons entraîné un modèle différent par demi-heure. Nous avons sélectionné 24 demi-heures sur 48 (les premières demi-heures de chaque heure), pour faciliter l'affichage des résultats : cela ne change rien aux méthodes étudiées. Nous obtenons ainsi les jeux de données :

- **d_ent_ouvre_i** pour $1\le i\le 24$, où l'on n'a gardé que les jours ouvrés dans **d_ent**, à l'heure $i$,
- **d_test_ouvre_i** pour $1\le i\le 24$, où l'on n'a gardé que les jours ouvrés dans **d_test**, à l'heure $i$.

## 2 Évaluation des modèles : quelques repères

Pour évaluer la performance de nos modèles, nous introduisons la classique perte quadratique (RMSE), et également l'erreur absolue moyenne en pourcentage (MAPE), que nous allons majoritairement exploiter en tant que grandeur "absolue" sans dimension :

$$\text{RMSE}(\varepsilon) = \sqrt{\frac{1}{T}\sum_{i=1}^T \varepsilon_i^2},$$
et
$$\text{MAPE}(Y,\hat{Y}) = 100 \times \frac{1}{T}\sum_{i=1}^T\left \lvert \frac{Y_i-\hat{Y}_i}{Y_i}\right \rvert.$$

Pour avoir une idée du positionnement de nos modèles en termes de performance, et de l'utilité de recourir au machine learning pour répondre à notre problème, nous pouvons nous comparer :

- au modèle naïf *à faire : je pensais à Load.48, est-ce que tu aurais d'autres idées ? _Je pense que c'est effectivement bien de prendre Load.48*
- aux prévisions RTE : sur le jeu de données test (année 2019), on calcule les MAPE suivants pour **Forecast_RTE_intraday** pour les jours ouvrés (en pourcentage, dans l'ordre croissant par heure)

```{r, echo=FALSE, include=TRUE}
mape_ouvre_RTE <- c()
for(i in c(1:H))
{
  mape_ouvre_RTE <- c(mape_ouvre_RTE, mape(eval(parse(text=paste("d_test_ouvre", i, sep="_")))$Load, eval(parse(text=paste("d_test_ouvre", i, sep="_")))$Forecast_RTE_intraday))
}
mape_ouvre_RTE
```


Soulignons que dans ses prévisions une heure à l'avance, RTE ne dispose que de prévisions météo, alors que nos modèles exploitent des valeurs de température effective, ce qui joue à notre avantage.
*c'est bien une heure à l'avance, pour intraday*

## 3 Forêts aléatoires

Un premier modèle assez adapté à notre problématique est celui des forêts aléatoires. Dans un premier temps, nous avons entraîné une forêt sur l'ensemble des données, sans la séparation des heures et des jours ouvrés/week-ends. Nous obtenons un MAPE de 1.8% sur les données test. Nous avons ensuite séparé les données conformément à la partie 1.3, et les résultats sont nettement meilleurs : c'est donc ce choix que nous développons dans cette partie. Nous avons utilisé le package *ranger*, et mis dans chaque modèle toutes les variables explicatives potentiellement pertinentes.

### 3.1 Choix des paramètres

Les paramètres à choisir pour définir nos forêts aléatoires sont les suivants.

- **mtry** : nombre de variables sélectionnées à chaque nouvelle coupure dans les arbres. Nous réalisons une validation croisée sur le modèle **d_ent_ouvre_1**, les autres modèles se comportant de façon similaire, et choisissons **mtry** = 8, car la courbe est à peu près constante à partir de cette valeur.


```{r, out.width='80%', fig.align='center', fig.show='hold', echo = F}
knitr::include_graphics('FIGURES/choix_mtry_forets.png')
```


- **min.node.size** : les fluctuations de RMSE induites par le changement de ce paramètre sont faibles, d'après la courbe ci-dessous. Nous laissons donc **min.node.size** à sa valeur par défaut (5).

```{r, out.width='80%', fig.align='center', fig.show='hold', echo = F}
knitr::include_graphics('FIGURES/choix_minnode_forets.png')
```


- **num.trees** : au vu de la courbe ci-dessous, nous choisissons de prendre 150 arbres, ce qui est un peu au-delà de la valeur où la courbe forme un coude, mais nous pouvons nous le permettre car nos jeux de données par heure ne sont pas très volumineux.

```{r, out.width='80%', fig.align='center', fig.show='hold', echo = F}
knitr::include_graphics('FIGURES/choix_ntree_forets.png')
```



### 3.2 Entraînement et évaluation des modèles

Une fois les paramètres choisis, nous pouvons entraîner nos modèles.

```{r, eval=FALSE, echo=TRUE}
formule_1 <- "Load ~ Temp_s99+toy+WeekDays+Temp_s95+Load.48+Christmas_break+Summer_break+DLS+Temp+Temp_s95_min+Temp_s95_max+Temp_s99_min+Temp_s99_max"

# calcul des modèles
for(i in c(1:H))
{
  assign(paste("foret_ouvre", i, sep="_"), ranger(formule_1, data=eval(parse(text=paste("d_ent_ouvre", i, sep="_"))), importance = "permutation", num.trees = 150, mtry=8))
}
```

```{r, echo=FALSE}
# calcul des prédictions
for(i in c(1:H))
{
  assign(paste("pred_foret_ouvre", i, sep="_"), predict(eval(parse(text=paste("foret_ouvre", i, sep="_"))), data = eval(parse(text=paste("d_test_ouvre", i, sep="_")))))
}
```

```{r, echo=FALSE, include=TRUE}
# affichage des MAPE
mape_ouvre <- c()
for(i in c(1:H))
{
  mape_ouvre <- c(mape_ouvre, mape(eval(parse(text=paste("d_test_ouvre", i, sep="_")))$Load, eval(parse(text=paste("pred_foret_ouvre", i, sep="_")))$predictions))
}

plot(mape_ouvre, main = "MAPE en fonction de l'heure, jours ouvrés (année 2019)", xlab = "Heure", ylab = "MAPE (%)", type="l", ylim= c(0,2), lab=c(5,4,0), col="blue")
lines(mape_ouvre_RTE, col="red")
legend(x="bottomright", legend=c("foret","RTE"), col=c("blue","red"), pch=c(15,15))
```


### 3.3 Importance des variables

Enfin, les forêts aléatoires nous permettent de visualiser l'importance des différentes variables explicatives, ci-dessous sur le modèle **foret_ouvre_1**.

```{r, echo=FALSE, include=TRUE}
vip(foret_ouvre_1, num_features=13)
```

On observe que la consommation de la veille et les différentes transformations de la température sont les variables déterminantes.

## 4 Modèles additifs généralisés (GAM)

Les GAM sont des modèles particulièrement adaptés à la prédiction de la consommation d'électricité. Nous utilisons des splines de régression cubique et l'équation suivante, où les degrés de liberté **k** ont été choisis à l'aide de la fonction **gam.check** sur le modèle **gam_ouvre_12**, en supposant que ces paramètres doivent être à peu près les mêmes pour chaque modèle.

```{r, eval=FALSE}
equation <- Load~s(Load.48,k=3, bs="cr")+s(Temp_s95, k=5, bs="cr")+s(Temp_s99, k=5, bs="cr")+s(Temp, k=5, bs="cr")+s(toy, k=35, bs="cr")+te(Load.48,Temp_s99)+WeekDays+DLS+Christmas_break+Summer_break+s(Temp_s99_min, k=5)+s(Temp_s99_max, k=5)+s(Temp_s95_min, k=5)+s(Temp_s95_max, k=5)
```



### Modèles variable par variable, pour optimiser la valeur de k

On vient de voir les forêts aléatoires qui nous ont données des résultats satisfaisant mais on peut espérer avoir de meilleurs résultats avec des modèles GAM bien paramétrés. Pour ce faire, on va dans un premier temps effectuer un GAM variable par variable pour comprendre leur influence sur la consommation et pour déterminer le degré de liberté optimal pour bien décrire les données sans pour autant faire un modèle trop complexe (même si compte-tenu du nombre de données à disposition, le risque d'overfitting est assez réduit). Avec les forêts et l'importance des variables, on a une guideline pour choisir lesquelles traiter en premier. Cette partie étant purement explicative, nous ne nous limitons pas à un modèle par heure pour simplifier l'interpretation qui, de toute manière, ne change pas si on observait les 24 résidus à chaque fois.


```{r, echo=FALSE}
g0 <- gam(Load~s(Load.48, k=3, bs="cr"), data=d_ent_ouvre)
plot(d_ent_ouvre$Load, g0$residuals, main = "Résidus pour g0", xlab = "Consommation (MWh)", ylab = "Résidus" )
fit0 <- getViz(g0, nsim = 50)
plot(sm(fit0, 1), n = 400 ) +labs(title="Lien entre Load.48 et la consommation",x="Load.48 (MWh)",y="Consommation (MWh)")+ l_points() + l_fitLine() + l_ciLine()

```

On a la confirmation que Load.48 est une feature essentielle pour la prévision puisque ce simple modèle (où f est une fonction quasi affine donc on peut prendre k=3) explique déjà 89% de la variance. Pour autant, les résidus ne semblent clairement pas suivre une loi normale centrée ce qui laisse présager d'amélioration.

```{r, echo=FALSE}
g2 <- gam(Load~s(Load.48, k=3, bs="cr")+s(Temp_s95_max, k=5, bs="cr")+s(Temp_s99_max, k=5, bs="cr"), data=d_ent_ouvre)
fit2 <- getViz(g2, nsim = 50)
plot(d_ent_ouvre$Load, g2$residuals, main="Résidus pour g2", xlab="Consommation (MWh)",ylab="Résidus", pch=16)
plot(sm(fit2, 2), n = 400) + l_points() + l_fitLine() + l_ciLine()+labs(title="Lien entre Temp_s95_max et la consommation",x="Temp_s95_max (MWh)",y="Consommation (MWh)")
plot(sm(fit2, 3), n = 400) + l_points() + l_fitLine() + l_ciLine()+labs(title="Lien entre Temp_s99_max et la consommation",x="Temp_s95_max (MWh)",y="Consommation (MWh)")
#summary(g2)

```
On obtient un R2 de près de 92% donc l'amélioration est notable, d'autant plus qu'on reste avec des degrés de fonctions assez simple (k=5). Dans le prochain GAM, on ajoute toutes les données discrète (heures (via "time of the day") et jours)

```{r, echo=FALSE}
g3 <- gam(Load~s(Load.48, k=3, bs="cr")+s(Temp_s95_max, k=5, bs="cr")+s(Temp_s99_max, k=5, bs="cr")+WeekDays+s(tod, k=24, bs="cr"), data=d_ent_ouvre)
plot(d_ent_ouvre$Load, g3$residuals, main = "Résidus pour g3", xlab = "Consommation (MWh)", ylab = "Résidus" )
fit3 <- getViz(g3, nsim = 50)
```


On obtient un R2 de plus de 97% et on remarque que même si on laisse un grand degré de liberté pour la dépendance en toy (k=48) cela n'améliore pas le modèle. Cela justifie le fait de faire un modèle par heure plutôt qu'un par demi-heure. 
On traite à part le cas de toy puisque le degré de liberté est moins facilement paramétrable, on décide donc de trouver k par cross validation :

```{r, echo=FALSE}
load("A_AFFICHER/cross_validation")
plot(K, rmseK, type='b', pch=20,main="",xlab="",ylab="")
points(K[12],rmseK[12],col="red")
title(main="Cross validation pour déterminer nombre de splines pour décrire la feature toy",xlab="dimension de la base de spline (k)",ylab="RMSE (obtenu par block")
legend(x="topright",legend=c("k=14"),col=c("red"),pch=c(20))
```
On choisit donc k=14 par technique du coude. On réutilisera une cross-validation pour définir la base de spline que l'on va utiliser au final. Mais avant ça on calcul le gam final avec les degrés de liberté obtenus dans cette partie


### Premier modèle GAM (sans découpage heure par heure)

Dans cette partie, on va utiliser l'équation obtenue précédemment à laquelle on va ajouter les features discrètes à savoir le jour de la semaine et si on se trouve pendant les vacances d'été ou de Noel. Mais avant de regarder les résultats d'un tel GAM sur les données de test, on va regarder par cross-validation quelle base de spline utiliser globalement.

```{r, echo=FALSE}
load("A_AFFICHER/splines_basis")
#tableau
knitr::kable(tableau,colors = c("black", "white"))
```

On note que les résultats sont sensiblement les mêmes pour thin plate regression et pour cubic regression et légèrmeent moins bon pour cyclic cubic regression mais compte-tenu de ces résultats, on choisit pour la suite d'utiliser des splines de regression cubique. Toutefois, on fait remarqué q'une étude précise nécessiterait de faire ce travail pour trouver la meilleure base de splines adaptée à chaque variable. Cependant nous n'avons pas jugé cela utile aux vues des faibles différences de performance. 

Désormais, on peut commencer à analyser les performances de notre "bon" GAM dans le cas où on ne fait pas encore un modèle par heure :

```{r, echo=FALSE}
equation<-Load~s(Load.48, k=3, bs="cr")+s(Temp_s95_max, k=5, bs="cr")+s(Temp_s99_max, k=5, bs="cr")+s(Temp_s95_min, k=5, bs="cr")+s(Temp_s99_min, k=5, bs="cr")+s(Temp_s95, k=5, bs="cr")+s(Temp_s99, k=5, bs="cr")+WeekDays+s(tod, k=24, bs="cr")+s(toy,k=14)+Christmas_break+Summer_break

gtot <- gam(equation, data=d_ent_ouvre)
plot(d_ent_ouvre$Load, gtot$residuals, main = "Résidus pour g3", xlab = "Consommation (MWh)", ylab = "Résidus" )
fittot <- getViz(gtot, nsim = 50)

```
On obtient une variance expliquée de l'ordre de 98% sans pour autant avoir un trop grand nombre de paramètres. De plus, les résidus ressemblent désormais bien à du bruit et donc semblent ne plus réellement contenir d'information.

On peut alors le tester sur l'année suivante pour voir si notre GAM possède de bonnes propriétés de généralisation.


```{r, echo=FALSE}
predictgam<-predict(gtot,newdata=d_test_ouvre)
plot(d_test_ouvre$Date,d_test_ouvre$Load, xlab = "Date", ylab = "Consommation (MWh)", type="l", lab=c(5,4,0), col="black") 
lines(d_test_ouvre$Date,d_test_ouvre$Forecast_RTE_intraday, xlab = "Date", ylab = "Consommation (MWh)", type="l", lab=c(5,4,0), col="blue") 
lines(d_test_ouvre$Date,predictgam, xlab = "Date", ylab = "Consommation (MWh)", type="l", lab=c(5,4,0), col="red")
legend(x="bottomright", legend=c("Donnée Réel","RTE","GAM"), col=c("black","blue","red"), pch=c(15,15,15))
```
On voit dans ce plot annuelle que notre prédiction semble globalement collé à la consommation réelle, pour autant cette échelle ne permet pas une analyse fine donc on va s'attarder plutôt sur un mois en particulier, ici le mois de Décembre.


```{r, echo=FALSE}
n <- 12048-48*21+1
m <- 12048
d_test_plot <- d_test_ouvre[n:m,]

predictgamdecembre<-predict(gtot,newdata=d_test_plot)
plot(d_test_plot$Load, xlab = "Date", ylab = "Consommation (MWh)", type="l", lab=c(5,4,0), col="black",axes=FALSE, ylim=c(min(d_test_plot$Load)-5000,max(d_test_plot$Load)+5000)) 
par(mar=c(5, 5, 4, 2) + 0.0) #ajustement de la marge gauche
axis(2, las=1)
lines(d_test_plot$Forecast_RTE_intraday, xlab = "", ylab = "Consommation (MWh)", type="l", lab=c(5,4,0), col="blue") 
lines(predictgamdecembre, xlab = "", ylab = "Consommation (MWh)", type="l", lab=c(5,4,0), col="red")
legend(x="bottomright",legend=c("Donnée Réel","RTE","GAM"), col=c("black","blue","red"), pch=c(15,15,15))
#axis.Date(1, at=d_test_plot$Load, labels=d_test_plot$Date)
seq<-seq(0,1000,1000/6)
axis(1, at = seq,
     labels = c("Jan", "Mar", "May", "Jul", "Sep", "Nov","Jan"),tck = -0.02,tcl = 0)
par(mgp = c(3, 0.5, 0))


```

###analyse plus fine : GAM avec découpage heure par heure

Les GAM sont des modèles particulièrement adaptés à la prédiction de la consommation d'électricité. 



```{r, echo=FALSE}
################## Cubic regression

# calcul des prédictions
for(i in c(1:H))
{
  assign(paste("pred_ouvre", i, sep="_"), predict(eval(parse(text=paste("gam_ouvre_cr", i, sep="_"))), newdata = eval(parse(text=paste("d_test_ouvre", i, sep="_")))))
}

# affichage des MAPE
mape_ouvre <- c()
for(i in c(1:H))
{
  mape_ouvre <- c(mape_ouvre, mape(eval(parse(text=paste("d_test_ouvre", i, sep="_")))$Load, eval(parse(text=paste("pred_ouvre", i, sep="_")))))
}

# mape_ouvre  # max = 1.55, min = 1.33

plot(mape_ouvre, main = "MAPE en fonction de l'heure, jours ouvrés (année 2019)", xlab = "Heure", ylab = "MAPE (%)", type="l", ylim= c(0,1.8), lab=c(5,4,0), col="blue")
lines(mape_ouvre_RTE, col="red")
legend(x="bottomright", legend=c("GAM","RTE"), col=c("blue","red"), pch=c(15,15))

```

Nous affichons l'attache aux données et la prédiction pour le modèle **gam_ouvre_12**.

```{r, echo=FALSE, include=TRUE}
# midi

rmse(d_test_ouvre_12$Load-pred_ouvre_12)
mape(d_test_ouvre_12$Load,pred_ouvre_12)

# affichage sur les données d'entraînement
d_plot_ouvre_12 <- filter(d_ent_ouvre_12, Year=="2018")
g_plot_ouvre_12 <- predict(gam_ouvre_cr_12, newdata = d_plot_ouvre_12)
plot(d_plot_ouvre_12$toy, d_plot_ouvre_12$Load, main = "Entraînement jours ouvrés à midi", xlab = "Date (année 2018)", ylab = "Consommation (MWh)", type="l", lab=c(5,4,0))
lines(d_plot_ouvre_12$toy, g_plot_ouvre_12, col='red')
legend(x="bottomleft", legend=c("données","gam_ouvre_12"), col=c("black","red"), pch=c(15,15))

# affichage sur les données test
plot(d_test_ouvre_12$toy, d_test_ouvre_12$Load, main = "Validation jours ouvrés à midi", xlab = "Date (année 2019)", ylab = "Consommation (MWh)", type="l", lab=c(5,4,0))
lines(d_test_ouvre_12$toy, pred_ouvre_12, col='red')
legend(x="bottomleft", legend=c("données","gam_ouvre_12"), col=c("black","red"), pch=c(15,15))

# affichage des différentes splines
fit1 <- getViz(gam_ouvre_cr_12, nsim = 50)
# ordre variables : Load48, Temp95, Temp99, Temp, toy
par(mfrow = c(3,2))

plot(sm(fit1, 1), n = 400) + l_points() + l_fitLine() + l_ciLine()
plot(sm(fit1, 2), n = 400) + l_points() + l_fitLine() + l_ciLine()
plot(sm(fit1, 3), n = 400) + l_points() + l_fitLine() + l_ciLine()
plot(sm(fit1, 4), n = 400) + l_points() + l_fitLine() + l_ciLine()
plot(sm(fit1, 5), n = 400) + l_points() + l_fitLine() + l_ciLine()

```

```{r, echo=FALSE, include=TRUE}
gam.check(gam_ouvre_cr_12)
```

*faire une figure avec attaches aux données et validation pour chaque type de modèle, pour pouvoir comparer*

*mettre le code des figures à part*

*étudier les résidus*

## 5 Boosting

Dans cette partie, nous utilisons les packages **gbm** et **xgboost** pour entraîner des modèles de boosting à base d'arbres de régression.

### 5.1 Boosting avec gbm

Pour chaque heure $i=0,\dots, 23$, nous faisons appel au code suivant.

```{r, eval=FALSE, echo=TRUE}
Ntree <- 1000

equation <- as.formula("Load ~ Temp_s99+toy+Lundi+Mardi+Mercredi+Jeudi+Vendredi+Temp_s95+Load.48+Christmas_break+Summer_break+DLS+Temp+Temp_s95_min+Temp_s95_max+Temp_s99_min+Temp_s99_max")

gbm(equation, distribution = "gaussian", data=eval(parse(text=paste("d_ent_ouvre", i, sep="_"))), n.trees = Ntree, interaction.depth = 10, n.minobsinnode = 5, shrinkage = 0.05, bag.fraction = 0.5, train.fraction = 1, keep.data = FALSE, n.cores = 4)
```

Les paramètres choisis sont essentiellement ceux recommandés par le module gbm. Une validation croisée montre que prendre **Ntree** de l'ordre de 100 suffirait, mais nous pouvons nous permettre de prendre 1000 arbres car le temps de calcul n'est pas très long ici. *vérifier qu'il n'y a pas de risque d'overfitting*

Nous traçons la courbe de MAPE à la fin de la partie, pour comparer directement avec **xgboost**.

### 5.2 Boosting avec xgboost

Le module **xgboost** est réputé pour avoir d'excellentes performances, et utilisé dans de nombreuses compétitions de machine learning. Nous appliquons pour chaque $i=0,\dots,23$ le code suivant, où nous gardons les mêmes covariables que pour **gbm**.

```{r, echo=TRUE, eval=FALSE}
xgboost(params=list(subsample=0.9, eta = 0.05, max.depth = 10, colsample_bytree=1), data = eval(parse(text=paste("d_xgb_ent", i, sep="_"))), label = eval(parse(text=paste("d_ent_ouvre", i, sep="_")))$Load, nthread = 4, objective = "reg:squarederror", nround = 1000, booster = "gbtree", verbose = 0)
```

À nouveau, les paramètres sont essentiellement ceux recommandés habituellement.

### 5.3 Performance des méthodes de boosting

Nous pouvons à présent tracer la courbe de MAPE des deux méthodes de boosting considérées, à comparer au MAPE des prévisions de RTE.

```{r, echo=FALSE}

plot(mape_ouvre_gbm, main = "MAPE en fonction de l'heure, jours ouvrés (année 2019)", xlab = "Heure", ylab = "MAPE (%)", type="l", ylim= c(0,1.8), lab=c(5,4,0), col="blue")
lines(mape_ouvre_RTE, col="red")
lines(mape_ouvre_xgb, col="pink")

```


## 6 Modèles mixtes

Dans les trois parties précédentes, nous avons entraîné successivement des forêts aléatoires, des GAM et un algorithme de boosting reposant sur des arbres de régression. Nous pouvons à présent essayer de conjuguer ces modèles, suivant une approche de stacking.

Nous allons exploiter les différentes splines apprises par nos modèles GAM, en les intégrant à notre jeu de données sous la forme de nouvelles variables. L'idée est ensuite d'entraîner un autre modèle (dans notre cas, une forêt, ou bien du boosting) sur cette base de donnée complétée. La performance de cette méthode est souvent meilleure que celle des forêts ou du boosting sur la base de donnée initiale, car les splines du GAM apportent une information plus facile à exploiter sur les corrélations entre variables.

### 6.1 Stacking forêts et GAM

Dans cette sous-partie, nous apprenons une forêt aléatoire sur la nouvelle base de données complétée par le GAM. Nous obtenons la courbe de MAPE suivante.

```{r, echo=FALSE}
for(i in c(1:H))
{
  assign(paste("pred_stack_ouvre", i, sep="_"), predict(eval(parse(text=paste("stack_ouvre", i, sep="_"))), data = eval(parse(text=paste("d_test_ouvre_bis", i, sep="_")))))
}

# affichage des MAPE
mape_ouvre <- c()
for(i in c(1:H))
{
  mape_ouvre <- c(mape_ouvre, mape(eval(parse(text=paste("d_test_ouvre_bis", i, sep="_")))$Load, eval(parse(text=paste("pred_stack_ouvre", i, sep="_")))$predictions))
}

# mape_ouvre

plot(mape_ouvre, main = "MAPE en fonction de l'heure, jours ouvrés (année 2019)", xlab = "Heure", ylab = "MAPE (%)", type="l", ylim= c(0,2), lab=c(5,4,0), col="blue")
lines(mape_ouvre_RTE, col="red")
legend(x="bottomright", legend=c("GAM+forets","RTE"), col=c("blue","red"), pch=c(15,15))
```

On observe que le modèle n'est malheureusement pas nettement meilleur que les forêts seules.

### 6.2 Stacking boosting et GAM

Dans cette sous-partie, nous appliquons nos méthodes de boosting par arbres (avec les packages gbm et xgboost) à la prédiction sur la nouvelle base de données contenant les splines du GAM. Comme précédemment, nous pouvons tracer la courbe de MAPE des modèles obtenus, en fonction de l'heure.

```{r, echo=FALSE}
plot(mape_ouvre_sgbm, main = "MAPE en fonction de l'heure, jours ouvrés (année 2019)", xlab = "Heure", ylab = "MAPE (%)", type="l", ylim= c(0,1.8), lab=c(5,4,0), col="blue")
lines(mape_ouvre_RTE, col="red")
lines(mape_ouvre_sxgb, col="pink")
legend(x="bottomright", legend=c("gbm_stack","RTE", "xgb_stack"), col=c("blue","red", "pink"), pch=c(15,15,15))

```



Même si les modèles obtenus ne dépassent pas nettement les précédents, et sont en particuliers moins bons que le GAM, nous les gardons en vue de réaliser une agrégation d'experts.



## 7 QGAM & prévision en loi

### 7.1 Motivations

Dans les précédantes partie du rapport, nous nous contentions de fournir "la meilleur prévision possible" mais on peut également être amené à vouloir prédire plus que ça. En effet, des problèmes pratique peuvent impliquer de vouloir obtenir une prévision d'un intervalle de confiance que ce soit pour des raisons de gestion de risque ou bien pour des problématiques d'analyse d'événements rares. Voici donc une liste non-exhaustive des raisons d'effectuer des prévisions en loi ce que nous allons faire tout au long de cette partie.

### 7.2 Présentation & une première approche naîve

Comme expliqué dans le paragraphe précédant, ce que l'on cherche à obtenir c'est une prévision des quantiles de la loi qui nous intéresse. Or si l'on se rappelle du fonctionnement des GAMS (cf partie 4), notre model est designé pour produire quelque chose de la forme $$ y_i=X_i\beta + f_1(x_{1,i})+ f_2(x_{2,i},x_{3,i})+...+ f_d(x_{d,i})+\epsilon_i $$ où $$\epsilon_i \sim \mathcal{N}(0,\,\sigma^{2})$$
Par conséquent, un model que l'on a naturellement envie de prendre pour prédire la loi qui nous intéresse serait de prendre les quantiles d'une variable d'Espérance la prédiction produite par le GAM et de Variance $\sigma^2$ que l'on obtiendrait en prenant par exemple l'estimateur empirique de la variance sur les données d'entraînement.


```{r, echo=FALSE , include=FALSE}
#load("/Users/guillaumeprincipato/Desktop/Orsay/Cours/Semestre\ 1/Projet\ ML\ Goude/ProjetDataMining/datapropre")
#load("/Users/guillaumeprincipato/Desktop/Orsay/Cours/Semestre\ 1/Projet\ ML\ Goude/ProjetDataMining/qgam_0.05...0.95")
load("MODELES/qgam.rda")
load("DONNEES/data.rda")

```

Dans cette partie, nous nous sommes limités à une seule heure pour des raisons de calcul qui ont vite tendance à être très élevé avec l'approche que l'on verre en 6.3

Le Gam que l'on regarde est fait à partir de l'equation suivante : 
Lequation <- Load ~ s(Load.48)+s(Temp_s95)+s(Temp_s99)+s(Temp)+s(toy)+s(Temp_s95_min)+s(Temp_s95_max)+s(Temp_s99_min)+s(Temp_s99_max)+Lundi+Mardi+Mercredi+Jeudi+Vendredi

et on peut vérifier que l'hypothèse de normalité des résidus est loin d'être délirante lorsqu'on regarde l'histogramme des résidus qui semble correspondre quasi parfaitement :

```{r, echo=FALSE}
data_train_12 <- d_ent_ouvre_12
data_test_12 <- d_test_ouvre_12
data_train<-data_train_12
data_test<-data_test_12

#On fait les prévisions avec qgam

qgam.preds <- sapply(Viz_12, predict, newdata = data_test)
qgam.pl <- pinLoss(data_test$Load, qgam.preds, qu=qu_target)


 #On fait les prévisions avec gam
equation <- Load ~ s(Load.48)+s(Temp_s95)+s(Temp_s99)+s(Temp)+s(toy)+s(Temp_s95_min)+s(Temp_s95_max)+s(Temp_s99_min)+s(Temp_s99_max)+Lundi+Mardi+Mercredi+Jeudi+Vendredi

gam_quantile<- gam(equation, data=data_train)
forecast<-predict(gam_quantile, newdata=data_test)
standart_dev<-sd(gam_quantile$residuals)

qgam_predicteurs<-list()
gam_normale_predicteurs<-list()
j<-1
for (qu in qu_target) {

  qgam_predicteurs[[j]]<-qgam.preds[, as.character(qu)]
  gam_normale_predicteurs[[j]]<-qnorm(c(qu),mean=forecast,sd=standart_dev)
  j<-1+j
}

j<-1


ratio_quantiles<-list()

for (qu in qu_target){
  qgam<-qgam_predicteurs[[j]] #90%
  gam_normale<-gam_normale_predicteurs[[j]]


  
  pinLossqgam <- pinLoss(data_test$Load, qgam, qu=qu,add = FALSE)
  pinLossgam_normale <- pinLoss(data_test$Load, gam_normale, qu=qu,add = FALSE)
  #pinLossqgam<-pinLossqgam[,3]

  #print(pinLossqgam/pinLossgam_normale)
  PinballLoss_qgam<-mean(pinLossqgam)
  PinballLoss_gam_normale<-mean(pinLossgam_normale)
  #print(PinballLoss_qgam)
  ratio<-PinballLoss_qgam/ PinballLoss_gam_normale
  ratio_quantiles[[j]]<-ratio
  j<-j+1
}

```

```{r, echo=FALSE}
#gam.check(gam_quantile)
type <- "deviance"  ## "pearson" & "response" are other valid choices
resid <- residuals(gam_quantile, type = type)
linpred <- napredict(gam_quantile$na.action, gam_quantile$linear.predictors)
observed.y <- napredict(gam_quantile$na.action, gam_quantile$y)

hist(resid, xlab = "Residuals", main = "Histogram of residuals")



```

Pour autant, si on s'attarde sur le diagramme quantile/quantile, on remarque que pour les valeurs extrêmes cela est moins vrai.

```{r,echo=FALSE}
split.screen(1:2)
screen(1)
qq.gam(gam_quantile, rep = 0, level = 0.9, type = type, rl.col = 2, 
       rep.col = "gray80")
screen(2)

plot(linpred, resid, main = "Resids vs. linear pred.", 
     xlab = "linear predictor", ylab = "residuals")
```


De plus, on doit remarquer que la variance des résidus ne semble pas être constante ce qui peut laisser imaginer des améliorations potentielles pour une méthode qui prend cette caractéristique en compte. On peut, pour conclure cette partie, afficher ce que donne cette méthode pour les quantiles allant de 5 % à 95 %.

```{r,echo=FALSE}
qu_targetbis<-c(0.05,0.45,0.50,0.55,0.9)
zfac <- factor(qu_targetbis)
mescouleurs <- rainbow(length(levels(zfac)))
plot(data_test$Date, data_test$Load, type='l',main = "", xlab="", ylab="", lwd=2)

lines(data_test$Date, gam_normale_predicteurs[[1]], col=mescouleurs[1]) 
lines(data_test$Date, gam_normale_predicteurs[[9]], col=mescouleurs[2]) 
lines(data_test$Date, gam_normale_predicteurs[[10]], col=mescouleurs[3]) 
lines(data_test$Date, gam_normale_predicteurs[[11]], col=mescouleurs[4]) 
lines(data_test$Date, gam_normale_predicteurs[[19]], col=mescouleurs[5]) 
title(main="Prévision quantile GAM + normale",xlab="Date",ylab="Consommation")
legend("top", inset = 0., pch =19, legend = levels(zfac), col = mescouleurs)

```
On remarque que les différents quantiles sont de simples translation de notre GAM ce qui est évident mais bon a rappelé dans le cas des quantiles GAM+normale

### 7.3 QGAM

Comme évoqué précédemment, le model décrit dans la partie précédante n'est qu'une approche naïve car dans notre cas, il est clair que la variance n'est pas la même selon si la consommation est importante ou non. Par conséquent, nous avons choisi d'utiliser des QGAMs qui est une méthode qui s’affranchit du cadre paramétrique dans lequel les GAMs étaient contraints en utilisant des méthodes de calibrations Bayesiennes (@Manual) ce qui va permettre à la variance de ne plus être une simple constante. Pour ce faire, le model est entraîné à partir d'une fonction de perte classique dès lors que l'on s'intéresse au quantile à savoir la Pinball Loss : 

```{r, echo=FALSE}
n <- 1000
x <- seq(0, 4, length.out = n)
plot(x, pinLoss(x, rep(2, n), qu = 0.5, add = FALSE),main="Pinball Loss", type = 'l', ylab = "loss")
j<-1

mescouleurs <- rainbow(length(levels(zfac)))

for (qu in qu_targetbis) {
  lines(x, pinLoss(x, rep(2, n), qu = qu, add = FALSE), col=mescouleurs[j]) 
  j<-j+1
}
j<-1
```
On entraîne alors nos QGAMs selon les mêmes features que notre GAM pour avoir une bonne base de comparaison. A noté que la compilation est logique compte-tenu du fait que le problème non-paramétrique soit évident plus complexe à résoudre qu'un "simple" GAM.
Pour visualiser le résultat, on peut là encore faire le même type de plot que dans la partie 5.2 et on remarque que les tracés ont l'air plus complexes et plus adaptés à la réalité des données. En effet, la courbe des quantiles les plus élevés à des variations de plus faibles amplitude que pour les autres quantiles. Une interprétation de ce phénomène pourrait-être que, comme cela est particulièrement visible en de juin à août, en été les consommations particulièrement élevé se propagent bien dans le temps, par exemple en période de canicule avec l'utilisation de la climatisation plusieurs jours de suite (ce n'est qu'une tentative d'interprétation).
```{r, echo=FALSE}
qu_targetbis<-c(0.05,0.45,0.50,0.55,0.9)
zfac <- factor(qu_targetbis)
mescouleurs <- rainbow(length(levels(zfac)))
plot(data_test$Date, data_test$Load, type='l',main = "", xlab="", ylab="", lwd=3)

lines(data_test$Date, qgam_predicteurs[[1]], col=mescouleurs[1]) 
lines(data_test$Date, qgam_predicteurs[[9]], col=mescouleurs[2]) 
lines(data_test$Date, qgam_predicteurs[[10]], col=mescouleurs[3]) 
lines(data_test$Date, qgam_predicteurs[[11]], col=mescouleurs[4]) 
lines(data_test$Date, qgam_predicteurs[[19]], col=mescouleurs[5]) 
title(main="Prévision quantile QGAM",xlab="Date",ylab="Consommation")
legend("top", inset = 0., pch =19, legend = levels(zfac), col = mescouleurs)
```
### 7.4 Comparaison entre la méthode naïve et les QGAM
Intuitivement, on se dit que QGAM doit donner de meilleurs résultats que GAM+normale mais reste à trouver un critère qui permette de les comparer. Dans un premier temps nous nous sommes dis que nous pouvions compter le nombre de données du dataset de test qui sont au dessus du quantile q mais cette approche ne mesure pas vraiment ce que l'on veut.
Nous avons donc décidé d'utiliser ici uniquement un critère plus pertinent à savoir la Pinball Loss sur les données d'entraînement et d'afficher le ratio entre les deux méthodes : $$ratio(quantile) = \frac{pinball(qgam(quantile))}{pinball(gam_+normale(quantile))}$$

```{r, echo=FALSE}
#Ratio à midi
plot(qu_target,ratio_quantiles,main = "Ratio", xlab="quantile", ylab="ratio",type="l")
y<-rep(1,length(qu_target))
lines(qu_target,y, type="l",col="blue", lty=2)
mean<-0
for (ratio in ratio_quantiles) {
  mean<-mean+ratio
}
#print(mean/19)
#ratio_quantiles
``` 

Comme prévu, QGAM donne de meilleur résultat pour environ tous les quantiles. A noter toutefois que pour les quantiles entre 15% et 35% les quantiles de la loi normale donnent des résultats légèrement meilleur (de l'ordre d'1 à 3%) mais cela n'est pas comparable aux écarts allant jusqu'à plus de 10% pour le reste des quantiles (plus les quantiles sont extrêmes plus la différence de performance est grande). On a d'ailleurs un gain en moyenne de l'ordre de 5% des QGAMs sur l'approche naïve ce qui n'est pas négligeable. Par conséquent, on peut conclure que lorsqu'on s'intéresse aux quantiles extrêmes, QGAM est nécessaire car c'est justement pour ces données extrêmes que l'hypothèse de variance constante n'est plus viable. En revanche, GAM+normale peut être utilisé notamment dans des modèles d’agrégation d'expert puisqu'il est assez performant pour des quantiles proches de la médiane.


### 7.5 Applications (Focus on 90%)

Dans cette dernière sous-partie, nous allons regarder un cas d'application pour un quantile extrême à savoir 90 %. Dans le cas de l'électricité, on peut facilement imaginer des applications pour calibrer le parc électrique de sortes à ce qu'il puisse produire à moindre coût pour des consommations "habituelles". Dans un premier temps, on affiche les quantiles sur les données de test.

```{r, echo=FALSE , include=FALSE}
#mauvais qgam

equation1 <- Load ~ s(Load.48)
qgam_l1<- mqgam(form = list(equation1,~ s(Temp)), data = data_train, qu = c(0.5,0.9))
Viz1 <- getViz(qgam_l1)
qgam.preds1 <- sapply(Viz1, predict, newdata = data_test)
```


```{r,echo=FALSE}
plot(data_test$Date, data_test$Load, type='l',main = "", xlab="", ylab="",col="black")
lines(data_test$Date, qgam_predicteurs[[18]], col="red") 
lines(data_test$Date, gam_normale_predicteurs[[18]], col="blue") 
lines(data_test$Date, qgam.preds1[,"0.9"], col="green") 
title(main="Comparaison des prédictions quantiles 90¨",xlab="Date",ylab="Consommation")
legend("top",legend=c("data test","GAM + normale 90%","QGAM 90%","mauvais QGAM (underfitting)"),col=c("black","red", "blue","green"), lty=1:1:1:1)

```

Ce graphique illustre plusieurs choses d'intéressantes, à commencer par le fait qu'un mauvais QGAM (qui ne prends en compte que la consommation de la veille) n'épouse pas la forme de la courbe notamment dans les cas où la consommation est faible et semble faire n'importe de quoi de Mars à Avril en oscillant énormément ce qui ne semble pas être justifié (les pics correspondent probablement au lundi et au vendredi où les consommations de la veille ne sont pas pertinente à cause du week-end) mais que sinon les deux autres coïncident globalement et semblent plausibles. On peut tout de même remarquer que le QGAM 90% semble plus proche des données réelles ce qui pourrait expliquer l'écart de 6% au niveau du ratio pour le quantile 90%. 

Pour conclure sur cette partie, bien que l'on ait dit que cela n'était pas vraiment pertinent, regardons la proportion des points du dataset de test qui sont au dessus de chacune des courbes : 

```{r, echo=FALSE}
verifquantile<-function(y,ychap)
{
  i<-1
  compteur<-0
  for (Load in y) {
    #print(Load)
    if (Load<=ychap[[i]]) {
      compteur<-compteur+1
    }
    
    i<-i+1
  }
  #print(i)
  #print(compteur)
  return (100*compteur/i)
  
}

verif1<-verifquantile(data_test$Load,qgam_predicteurs[[18]])
verif2<-verifquantile(data_test$Load,gam_normale_predicteurs[[18]])
verif3<-verifquantile(data_test$Load,qgam.preds1[,"0.9"])

proportion_au_dessus<-c(verif1,verif2,verif3)
verif_name<-c("QGAM","GAM+normale","mauvais QGAM")
dataframe<-as.data.frame(cbind(verif_name,proportion_au_dessus))

knitr::kable(dataframe,colors = c("black", "white"))
#dataframe
```

Cela ne va pas particulièrement dans le sens des observations faites précédemment ce qui confirme le fait que cette mesure est assez mauvais indicateur de la pertinence des quantiles. En effet, le mauvais QGAM à la proportion de point la plus proche du quantile que l'on vise bon tandis que le bon QGAM est le plus mauvais selon ce critère. Par ailleurs on note que pour les grands quantiles et donc les consommations élevés, nos modèles semblent sous-estimer leur nombre. A l'inverse, pour les faibles quantiles (10%) on trouve une proportion de plus de 15%. On peut chercher une piste pour expliquer cette observation dans un changement de tendance en fonction de l'année invisible en moyenne mais significative pour les valeurs extrêmes.
##Référence

```{r}
d_tot<-rbind (data_train_12,data_test_12)
plot(d_tot$Date,d_tot$Load,type="l",main = "", xlab="", ylab="", lwd=2)
lines(data_test_12$Date,data_test_12$Load, col="red")
title(main="Recherche de tendance",xlab="Date",ylab="Consommation")
legend("top",legend=c("data test","data train"),col=c("black","red"), lty=1:1)
```
On Voit effectivement que nous testons nos données sur une année avec assez peu de consommation extrêmement élevés et à l'inverse plus de consommation très faibles. Pour autant, il n'y a clairement pas de tendance qui semblait annoncer cela et par conséquent on reste sur notre hypothèse selon laquelle l'année n'est pas une feature exploitable. 


## 8 Agrégation d'experts : va-t-on battre RTE ?





@Manual{R-base,
  title = {qgam: Bayesian Nonparametric Quantile Regression Modeling in R},
  author = {{Matteo Fasiolo, Simon N. Wood, Margaux Zaffran, Raphaël Nedellec, Yannig Goude}},
  year = {2021},
  url = {https://www.jstatsoft.org/article/view/v100i09},
}






